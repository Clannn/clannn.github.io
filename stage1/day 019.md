review: 004 012 015 017 018

# Game Engine Architecture

## Memory Architecture

#### The Memory Gap

​	The access latency of main memory is extremely high relative to the latency of executing a single instruction. (On an Intel Core i7, a instruction takes between one and 10 cycles and an access to main RAM can take 500 cycles.)  This ever-increasing discrepancy between CPU speeds and memory access is called the *memory gap*.  Some techniques used to work around memory gap following:

- placing smaller, faster memory banks closer to the CPU core, so that frequently-used data can be accessed more quickly.
- “hiding” memory access latency by arranging for the CPU to do other useful work while waiting for a memory operation to complete.
- minimizing accesses to main memory by arranging a program’s data in as efficient a manner as possible.

#### Register Files

​	Registers are typically implemented using multi-ported static RAM, usually with dedicated ports for read and write operations. What’s more, the register file is typically located immediately adjacent to the circuitry for the ALU that uses it.

### Memory Cache Hierarchies

​	In a cache hierarchy , a small but fast bank of RAM called the $level\ 1$ ($L1$) cache is placed very near to the CPU core. A larger but somewhat slower $level\ 2$ ($L2$) cache. Some machines even have larger but more-distant $L3$ or $L4$ caches. These caches work in concert to automatically retain copies of the most frequently-used data.

​	If the data requested by the CPU is already in the cache, it can be provided to the CPU very quickly—on the order of tens of cycles. 

#### Cache Lines

​	To take advantage of locality of reference, memory caching systems move data into the cache in contiguous blocks called cache lines rather than caching data items individually.

​	For example, If the program is accessing the data members of an instance of a class. However, the cache controller doesn’t just read that one member—it actually reads a larger contiguous block of RAM into the cache.

#### Mapping Cache Lines to Main RAM Addresses

​	There is a simple one-to-many correspondence between memory addresses in the cache and memory addresses in main RAM. As a example, let’s say that our cache is $32\ KiB$ in size, and that cache lines are $128\ bytes$ each. Let’s further assume that main RAM is $256\ MiB$ in size. So main RAM is 8192 times as big as the cache, that means we need to overlay the address space of the cache onto the main RAM address space 8192 times. Given any address in main RAM, we can find its address in the cache by taking the main RAM address modulo the size of the cache.

#### Addressing the Cache

​	The cache can only deal with memory addresses that are aligned to the cache line size. Hence we need to convert our byte’s address into a *cache line index*.

​	Consider a cache that is $2^M$ bytes in total size, containing lines that are $2^n$ in size. The $n$ least-significant bits of the main RAM address represent the *offset* of the byte within the cache line. The $(M−n)$ middle-significant bits become the cache line index. All the remaining bits tell us which cache-sized block in main RAM the cache line come from. The block index is known as the *tag*.

​	The cache also maintains a small table of tags, one for each cache line. This allows the caching system to keep track of from which main RAM block each line in the cache came. For 32-bit memory address: 

![image-20220829134057855](C:\Users\Clan\OneDrive\桌面\study\研一\studys\day by day\pictures\image-20220829134057855.png)

​	When the CPU issues a read operation: The main RAM address is converted into an offset, line index and tag. The corresponding tag in the cache is checked, using the line index to find it. If the tag in the cache matches the requested tag, the line index is used to retrieve the line-sized chunk of data from the cache, and the offset is used to locate the desired byte within the line. If the tags do not match, the appropriate line-sized chunk of main RAM is read into the cache, and update the corresponding tag.

#### Set Associativity and Replacement Policy

​	The simple mapping between cache lines and main RAM addresses described above is known as a *direct-mapped* cache. When a cache miss occurs, CPU must evicts a cache line. This can result in pathological case. For example, two unrelated main memory blocks might keep evicting one another. We can obtain better average performance if each main memory address can map to two or more distinct lines in the cache. 

​	Once we have more than one “cache way ,” the cache controller is faced with a problem: When a cache miss occurs, which of the “ways” should be evicted? The answer to this question differs between CPU designs, and is known as the CPU’s *replacement policy*. One popular policy is eviction always affects the “way” that is *not most-recently used* one.

# Vulkan Tutorial

## Instance

​	To create an instance we’ll first have to fill in a struct with some information about our application. **This data is technically optional.**

```c++
VkApplicationInfo appInfo{};
appInfo.sType = VK_STRUCTURE_TYPE_APPLICATION_INFO;
appInfo.pApplicationName = "Hello Triangle";
appInfo.applicationVersion = VK_MAKE_VERSION(1, 0, 0);
appInfo.pEngineName = "No Engine";
appInfo.engineVersion = VK_MAKE_VERSION(1, 0, 0);
appInfo.apiVersion = VK_API_VERSION_1_3;
```

​	This next struct is **not optional** and tells the Vulkan driver which global extensions and validation layers we want to use.

```c++
VkInstanceCreateInfo createInfo{};
createInfo.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;
createInfo.pApplicationInfo = &appInfo;
//specify the desired global extensions(for now, just glfw window needed extensions)
uint32_t glfwExtensionCount = 0;
const char** glfwExtensions;
glfwExtensions = glfwGetRequiredInstanceExtensions(&glfwExtensionCount);
createInfo.enabledExtensionCount = glfwExtensionCount;
createInfo.ppEnabledExtensionNames = glfwExtensions;
//for now, we don't need validation layer. what the meaning of Layer?
createInfo.enabledLayerCount = 0;

vkCreateInstance(&createInfo, nullptr, &instance)
```

## Validation layers

​	You can simply enable validation layers for debug builds and completely disable them for release builds. Vulkan does not come with any validation layers built-in, but the LunarG Vulkan SDK provides a nice set of layers that check for common errors. 

​	Just like extensions, validation layers need to be enabled by specifying their name. All of the useful standard validation is bundled into a layer included in the SDK that is known as `VK_LAYER_KHRONOS_validation`.

​	validation layers should be checked by ourselves (why? when create instance, the validation layers will be checked), using the `vkEnumerateInstanceLayerProperties` function.

### Message callback

​	The validation layers will print debug messages to the standard output by default, we can also handle them ourselves by providing an explicit callback. To set up a callback in the program to handle messages, we have to set up a debug messenger with a callback using the `VK_EXT_debug_utils` extension.

​	The callback function looks like:

```c++
static VKAPI_ATTR VkBool32 VKAPI_CALL debugCallback(
    VkDebugUtilsMessageSeverityFlagBitsEXT messageSeverity,
    VkDebugUtilsMessageTypeFlagsEXT messageType,
    const VkDebugUtilsMessengerCallbackDataEXT* pCallbackData,
    void* pUserData)
{
    std::cerr << "validation layer: " << pCallbackData->pMessage << std::endl;
    
    return VK_FALSE;
}
```

​	The first parameter specifies the severity of the message, which is one of the following flags:

- `..._VERBOSE_BIT_EXT`:  Diagnostic message
- `..._INFO_BIT_EXT`: Informational message
- `..._WARNING_BIT_EXT`: is not necessarily but an error
- `..._ERROR_BIT_EXT`: is invalid error and may cause crashes

​	The `messageType` can have the following values:

- `..._GENNERAL_BIT_EXT`: Some event has happened that is unrelated to the specification or performance
- `..._VALIDATION_BIT_EXT`: Event has happened that violates the specification or indicates a possible mistake
- `..._PERFORMANCE_BIT_EXT`:  Potential nonoptimal use of Vulkan

​	The `pCallbackData` parameter containing the details of the message itself, with the most important members being:

- `pMessage`: debug message
- `pObjects`: Array of Vulkan object handles related to the message
- `objectCount`: Number of objects in array

​	The `pUserData` parameter contains a pointer that was specified during the setup of the callback.

​	The callback returns a boolean that indicates if the Vulkan call that triggered the validation layer message should be aborted. You should always return `VK_FALSE`.

​	Even the debug callback in Vulkan is managed with a handle that needs to be explicitly created and destroyed. Such a callback is part of a *debug messenger* and you can have as many of them as you want.



# 基于物理的建模与动画

$P_{94}-P_{99}$

# C++ Primer

$P_{336}-P_{352}$

